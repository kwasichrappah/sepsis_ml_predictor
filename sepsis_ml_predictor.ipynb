{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `BUILDING A ML MODEL TO PREDICT SEPSIS IN PATIENTS` \n",
    "#### Using the CRISP-DM framework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Business Understanding`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal/Description\n",
    "# To create a machine learning model to predict the sepsis in a patient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# #### `Null Hypothesis`\n",
    "# There is no relationship between a tech savvy customer and the customer retention\n",
    "# \n",
    "# #### `Alternate Hypothesis`\n",
    "# There is a relationship between a tech savvy customer and the customer retention\n",
    "# \n",
    "# ###### NB: A tech savvy person is someone who has online security or device protection or both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ### `Key Metrics and Success Criteria`\n",
    "# \n",
    "# The success of this poject will be evaluated based on several key metrics and success criteria including;\n",
    "# \n",
    "# • Model Accuracy : The ability of the machine learning model to accurately predict customer churn.\n",
    "# \n",
    "# • Model Interpretability : The degree to which the model's predictions and insights can be understood and utilized by stakeholders.\n",
    "# \n",
    "# • Business Impact : The effectiveness of retention strategies implemented based on the model's recommendations in reducing customer churn rates and improving overall customer satisfaction and retention.\n",
    "# \n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# #### `Analytical Questions`\n",
    "# - How does tenure and monthly charge affect customer churn?\n",
    "# - What is the likelihood of a customer with online security and protection to churn?\n",
    "# - What is the relationship between the type of contract and the likelihood of a customer churn?\n",
    "# - Do customers with dependents and internet security likely to Churn?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## `Data Understanding`\n",
    "\n",
    "# %% [markdown]\n",
    "# #### Data Source\n",
    "# The data was sourced from a Telecommunication company and divided into three (3) parts :\n",
    "# - 3000 rows as the training data\n",
    "# - 2000 rows as the evaluation data \n",
    "# - 2000 rows as the test data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ### `Issues`\n",
    "# - Some columns have multiple adjectives of the same word. eg no,no internet service,false \n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# #### Data Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ##### `Libraries`\n",
    "\n",
    "# %%\n",
    "#Libraries imported\n",
    "import sqlalchemy as sa\n",
    "import pyodbc  \n",
    "from dotenv import dotenv_values \n",
    "import pandas as pd\n",
    "from scipy import stats \n",
    "from scipy.stats import kruskal\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "#Filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import * #train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC \n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "#for balancing dataset\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "#for feature selection\n",
    "from sklearn.feature_selection import mutual_info_classif,SelectKBest\n",
    "#Crossvalidation for hyper parameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#joblib for model persit\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import * \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ##### `Accessing the second set of data in CSV format`\n",
    "\n",
    "# %%\n",
    "##Accessing the second set of data \n",
    "csv_df = pd.read_csv(\"data\\\\LP2_Telco-churn-second-2000.csv\")\n",
    "csv_df.info()\n",
    "\n",
    "# %%\n",
    "# Describing the Dataframe\n",
    "csv_df.describe(include='all').T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ##### `Merging the Two Dataframes`\n",
    "\n",
    "# %%\n",
    "com_df=pd.concat([sql_df,csv_df],ignore_index=True)\n",
    "com_df.head(5)\n",
    "com_df.shape\n",
    "\n",
    "# %%\n",
    "#Checking the datatypes of the columns\n",
    "datatypes = com_df.dtypes\n",
    "datatypes\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### Converting the TotalCharges datatype to float64\n",
    "\n",
    "# %%\n",
    "#Converting TotalCharges column to numeric\n",
    "com_df['TotalCharges'] = pd.to_numeric(com_df['TotalCharges'], errors='coerce')\n",
    "com_df=com_df.reset_index()\n",
    "\n",
    "# %%\n",
    "# Checking the Null value\n",
    "com_df.isnull().sum()\n",
    "\n",
    "# %%\n",
    "com_df.head(5)\n",
    "data=com_df.copy()\n",
    "\n",
    "# %%\n",
    "#Dropping the index column\n",
    "com_df = com_df.drop(['index'], axis = 1 )\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### Replacing all negatives with False and positives with True\n",
    "\n",
    "# %%\n",
    "com_df.replace(['No','No internet service','false','No phone service'], \"False\", inplace = True)\n",
    "\n",
    "com_df.replace('Yes',\"True\", inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "com_df['SeniorCitizen'] = np.where(com_df['SeniorCitizen'] == 1, True, False)\n",
    "\n",
    "\n",
    "# %%\n",
    "com_df.InternetService.replace('false','None')\n",
    "\n",
    "# %%\n",
    "datatypes = com_df.dtypes\n",
    "datatypes\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### Making the True/False to Boolean\n",
    "\n",
    "# %%\n",
    "com_df.replace({'True': True, 'False': False}, inplace = True)\n",
    "\n",
    "# %%\n",
    "com_df.to_csv(\"data/customer_churn_merged\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Univariate Analysis\n",
    "\n",
    "# %%\n",
    "# Distribution of the variables\n",
    "com_df.hist(density = True,figsize = (20, 15), facecolor = 'lightgreen', alpha = 0.75,grid = False)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Visualize the distribution of categorical columns\n",
    "categoricals = [column for column in com_df.columns if com_df[column].dtype == \"O\"]\n",
    "for column in categoricals:\n",
    "        if column not in ['customerID']:\n",
    "                fig = px.histogram(com_df, x = com_df[column], text_auto = True,color = column,\n",
    "                               title = f\"Distribution of customers based on {column}\")\n",
    "                fig.update_layout(uniformtext_minsize = 8, uniformtext_mode = 'hide', xaxis_tickangle = -45)\n",
    "                fig.show()\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# #### OBSERVATION\n",
    "# - The Gender is evenly distributed \n",
    "# - Over 50% of all contract types are month-on-month basis\n",
    "# - Electronic Check is the most used,covering 30% of all payment methods\n",
    "# \n",
    "\n",
    "# %%\n",
    "fig = plt.figure(figsize = (5, 4))\n",
    " \n",
    "# Creating plot\n",
    "plt.boxplot(com_df.tenure)\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "fig = plt.figure(figsize = (5, 4))\n",
    " \n",
    "# Creating plot\n",
    "plt.boxplot(com_df.MonthlyCharges)\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Bivariate Analysis\n",
    "\n",
    "# %%\n",
    "# Summarizing the relationships between the variables with a heatmap of the correlations\n",
    "correlation_matrix = com_df.corr(numeric_only = True)\n",
    "plt.figure(figsize = (10, 8))\n",
    "sns.heatmap(correlation_matrix, annot = True,cmap = 'vlag')\n",
    "plt.title(\"Correlation heatmap of the Telecom Dataset\")\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "#  ## `Answering the Analytical Questions`\n",
    "# \n",
    "\n",
    "# %% [markdown]\n",
    "# ##### `How does tenure and monthly charge affect customer churn?`\n",
    "# \n",
    "\n",
    "# %%\n",
    "\n",
    "bins = [ 10, 30, 50,70]\n",
    "df = com_df\n",
    "labels = ['Newbie', 'Young', 'Oldies']\n",
    "df['tenure Group'] = pd.cut(df['tenure'], bins = bins, labels = labels)\n",
    "streamers = com_df.groupby(['tenure Group','Churn'])['MonthlyCharges'].mean().sort_values(ascending = True)\n",
    "\n",
    "streamers.plot(kind='bar', title = 'How does tenure and monthly charge affect customer churn?', figsize = (10,6), cmap='Dark2', rot = 30)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# #### OBSERVATION\n",
    "# - New,Existing and Old Customers with higher charges for software usage are the ones churning.\n",
    "# - There has to be a loyalty promotion for old customers to lock in the old customers.\n",
    "# - There can also be a signup discount to new customers to lock them in on the software.\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### `What is the likelihood of a customer with online security and device protection to churn?`\n",
    "# \n",
    "\n",
    "# %%\n",
    "cust_retention = com_df.groupby(['OnlineSecurity','DeviceProtection'])['Churn'].count().sort_values(ascending = True)\n",
    "cust_retention.plot(kind = 'bar', title = 'The likelihood of a customer with online security and device protection to churn', figsize = (10,6), cmap = 'Dark2', rot = 30)\n",
    "\n",
    "# %% [markdown]\n",
    "# #### OBSERVATION\n",
    "# - Customers with no security at all are more likely to Churn. \n",
    "# - Basic cybersecurity can be done to curb customer doubt to reduce Churn.\n",
    "\n",
    "# %% [markdown]\n",
    "# #### `What is the relationship between the type of contract and the likelihood of a customer churn?`\n",
    "# \n",
    "\n",
    "# %%\n",
    "cust_contract = com_df.groupby('Contract')['Churn'].count().sort_values(ascending = True)\n",
    "cust_contract.plot(kind = 'bar', title = 'The relationship between the type of contract and the likelihood of a customer churn', figsize = (10,6), cmap = 'Dark2', rot = 30)\n",
    "\n",
    "# %% [markdown]\n",
    "# #### OBSERVATION\n",
    "# - Month-to-Month Customers are more likely to churn as they are likely to be floating users.\n",
    "\n",
    "# %% [markdown]\n",
    "# #### `Do customers with dependents and internet security likely to Churn?`\n",
    "\n",
    "# %%\n",
    "cust_contract = com_df.groupby(['OnlineSecurity','Dependents'])['Churn'].count().sort_values(ascending = True)\n",
    "cust_contract.plot(kind='bar', title = 'Do customers with dependents and internet security likely to Churn?', figsize = (10,6), cmap = 'Dark2', rot = 30)\n",
    "\n",
    "# %% [markdown]\n",
    "# #### OBSERVATION\n",
    "# Customers with both Online Security and Dependents are less likely to churn.\n",
    "\n",
    "# %%\n",
    "com_df.isnull().sum()\n",
    "\n",
    "\n",
    "# %%\n",
    "#Dropping Empty rows\n",
    "com_df = com_df.dropna(subset=['OnlineSecurity','OnlineBackup','DeviceProtection','MultipleLines','TotalCharges','Churn'],axis = 0)\n",
    "\n",
    "# %%\n",
    "#finding duplicates\n",
    "duplicate = com_df[com_df.duplicated()]\n",
    "duplicate.shape\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### OBSERVATION \n",
    "# No duplicates found\n",
    "\n",
    "# %% [markdown]\n",
    "# #### `HYPOTHESIS`\n",
    "\n",
    "# %%\n",
    "#Checking Normality of the data \n",
    "\n",
    "def check_normality(data,name):\n",
    "    test_stat_normality, p_value_normality = stats.shapiro(data)\n",
    "    print(\"p value:%.20f\" % p_value_normality)\n",
    "    if p_value_normality < 0.05:\n",
    "        print(f\"Reject null hypothesis >> The data for {name} is not normally distributed\")\n",
    "    else:\n",
    "        print(f\"Fail to reject null hypothesis >> The data for {name} is normally distributed\")\n",
    "\n",
    "# %%\n",
    "#Hypothesis\n",
    "\n",
    "df_tech = com_df.loc[com_df.OnlineSecurity & com_df.DeviceProtection]\n",
    "online = com_df.loc[com_df.OnlineSecurity]\n",
    "device = com_df.loc[com_df.DeviceProtection]\n",
    "\n",
    "\n",
    "# %%\n",
    "#Normality Checks\n",
    "check_normality(df_tech.TotalCharges,'Online Security and Device Protection')\n",
    "check_normality(online.TotalCharges,'Online Security')\n",
    "check_normality(device.TotalCharges,'Device Protection')\n",
    "\n",
    "# %%\n",
    "#Using the P-Levene to test the Hypothesis\n",
    "stat, pvalue_levene = stats.levene(df_tech.TotalCharges, online.TotalCharges,device.TotalCharges )\n",
    "\n",
    "print(\"p value:%.10f\" % pvalue_levene)\n",
    "if pvalue_levene < 0.05:\n",
    "    print(\"Reject null hypothesis >> The variances of the samples are different.\")\n",
    "else:\n",
    "    print(\"Fail to reject null hypothesis >> The variances of the samples are same.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### Observation \n",
    "# - Data samples are not normally distributed\n",
    "# - The variances of the samples are different\n",
    "# - Therefore a Non-Parametric test must be done (Kruskal Test)\n",
    "\n",
    "# %%\n",
    "#Kruskal Test\n",
    "\n",
    "stat, p = kruskal(df_tech.TotalCharges, online.TotalCharges,device.TotalCharges)\n",
    "print('Statistics=%.3f, p=%.15f' % (stat, p))\n",
    "\n",
    "if p > 0.05:\n",
    " print('All sample distributions are the same (fail to reject H0)')\n",
    "else:\n",
    " print('One or more sample distributions are not equal distributions (reject null Hypothesis)')\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### OBSERVATION\n",
    "# Reject the null Hypothesis\n",
    "\n",
    "# %% [markdown]\n",
    "# ### `Data preparation`\n",
    "\n",
    "# %% [markdown]\n",
    "# #### Feature Correlation and Selection\n",
    "\n",
    "# %%\n",
    "# Summarize the relationships between the variables with a heatmap of the correlations\n",
    "correlation_matrix = df.corr(numeric_only= True).round(3)\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "plt.figure(figsize = (10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True,cmap='vlag',mask=mask)\n",
    "plt.title(\"Correlation heatmap of the dataset\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "\n",
    "df.head(1)\n",
    "df1 = data.drop(columns=['index','customerID','gender','TotalCharges'],axis=1)\n",
    "\n",
    "# %%\n",
    "# Dropping row with null value\n",
    "df1.dropna(axis = 0, inplace = True)\n",
    "\n",
    "# %%\n",
    "def str_convert(df,column_name):\n",
    "    df[column_name]=df[column_name].replace({1: 'Yes', 0: 'No'})\n",
    "\n",
    "    return df\n",
    "\n",
    "# %%\n",
    "df1 = str_convert(df1,'SeniorCitizen')\n",
    "\n",
    "\n",
    "df1=df1.map(lambda x: 'Yes' if x == True else 'No' if x == False else x)\n",
    "\n",
    "\n",
    "df1['tenure'] = pd.to_numeric(df1['tenure'], errors = 'coerce', downcast = 'integer')\n",
    "\n",
    "\n",
    "# %%\n",
    "def cleaner (df):\n",
    "    df = df.drop(columns=['customerID','gender','TotalCharges'],axis=1)\n",
    "    df['SeniorCitizen']=df['SeniorCitizen'].replace({1: 'Yes', 0: 'No'})\n",
    "    df=df.map(lambda x: 'Yes' if x == True else 'No' if x == False else x)\n",
    "    df['tenure'] = pd.to_numeric(df['tenure'], errors = 'coerce', downcast = 'integer')\n",
    "\n",
    "    return df\n",
    "\n",
    "# %% [markdown]\n",
    "# #### `Distribution of the dependent variable`\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### Dataset classification\n",
    "# \n",
    "# - Checking to see if the binary dependent variables are evenly distributed or not \n",
    "# - With the current levels of disparity between the two classes what stratification method will be best\n",
    "# \n",
    "\n",
    "# %%\n",
    "# Separate majority and minority classes\n",
    "df1_stay = df1[df1.Churn== 'No']\n",
    "df1_left = df1[df1.Churn==\"Yes\"]\n",
    "\n",
    "print((len(df1_stay)/len(df1)),(len(df1_left)/len(df1)))\n",
    "print(len(df1_left))\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### Observation\n",
    "# - About 70% of the customers stayed as compared to the customers that left therefore the churned customers represent the minority group\n",
    "# - Using undersampling means there will a huge loss of the majority class to balance the data\n",
    "# - Using oversampling means that there will  be a too many duplicates of the minority class in the balanced data \n",
    "# - For this dataset, it will be best to use SMOTE to balance the dataset\n",
    "\n",
    "# %%\n",
    "df1.dtypes\n",
    "\n",
    "# %%\n",
    "df1.head(4)\n",
    "\n",
    "# %% [markdown]\n",
    "# #### `Modeling`\n",
    "\n",
    "# %%\n",
    "df1.dtypes\n",
    "\n",
    "# %%\n",
    "# Dropping row with null value\n",
    "df1.dropna(axis = 0, inplace = True)\n",
    "\n",
    "# %%\n",
    "X=df1.drop(columns=['Churn'],axis=1)\n",
    "y=df1['Churn'].replace({'Yes': 1, 'No': 0})\n",
    "\n",
    "\n",
    "# %%\n",
    "# Looking at the descriptive statistics of the columns with categorical values\n",
    "cats = [column for column in X.columns if (X[column].dtype == \"O\")]\n",
    "print(\"Summary table of the Descriptive Statistics of Columns with Numeric Values\")\n",
    "df1[cats].describe(include=\"all\")\n",
    "\n",
    "# %%\n",
    "# Looking at the descriptive statistics of the columns with numeric values\n",
    "numerics = [column for column in X.columns if (X[column].dtype != \"O\")]\n",
    "print(\"Summary table of the Descriptive Statistics of Columns with Numeric Values\")\n",
    "df1[numerics].describe()\n",
    "\n",
    "# %%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = 27)\n",
    "\n",
    "# %%\n",
    "y_train_encoded = pd.to_numeric(y_train)\n",
    "y_test_encoded = pd.to_numeric(y_test)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### `Making pipelines`\n",
    "# \n",
    "\n",
    "# %%\n",
    "scaler = StandardScaler()\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "# putting numeric columns to scaler and categorical to encoder\n",
    "num_transformer = Pipeline(steps = [\n",
    "     ('num_imputer', SimpleImputer(strategy = 'median')),\n",
    "    ('num', scaler)\n",
    "])\n",
    "cat_transformer = Pipeline(steps = [\n",
    "   ('cat_imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('cat', encoder)\n",
    "])\n",
    "\n",
    "\n",
    "# %%\n",
    "# getting together our scaler and encoder with preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "      transformers = [('num', num_transformer , numerics),\n",
    "                    ('cat', cat_transformer , cats),\n",
    "     \n",
    "                    ])\n",
    "\n",
    "# %%\n",
    "#Calling the models of interest\n",
    "\n",
    "log_mod =  (LogisticRegression(random_state = 27 ))\n",
    "svc_mod = SVC(random_state=27,probability= True)\n",
    "\n",
    "catboost_mod = (CatBoostClassifier(random_state=27, verbose = False))\n",
    "xgboost_mod = XGBClassifier(random_state=27)\n",
    " \n",
    "\n",
    "# %%\n",
    "# Create a dictionary of the model pipelines\n",
    "all_models_pipelines = {\"Logistic_Regressor\": (LogisticRegression(random_state = 27 )),\n",
    "              \"SVM\": SVC(random_state = 27,probability = True),\n",
    "              \"CatBoost\": (CatBoostClassifier(random_state=27, verbose = False)),\n",
    "              \"Xgboost\":XGBClassifier(random_state=27)\n",
    "              }\n",
    "    \n",
    "\n",
    "# %%\n",
    "# Create a function to model and return comparative model evaluation scores,perform the SMOTE on each model pipeline,to calculate and compare accuracy\n",
    "\n",
    "def evaluate_models(model_pipelines = all_models_pipelines, X_test = X_test, y_test = y_test_encoded):\n",
    "\n",
    "\n",
    "    # Dictionary for trained models\n",
    "    trained_models = dict()\n",
    "\n",
    "    # Create a dataframe matrix to all pipelines\n",
    "    all_confusion_matrix = []\n",
    "    \n",
    "    \n",
    "    # List to receive scores\n",
    "    performances = []\n",
    "    for name, model_pipeline in model_pipelines.items():\n",
    "        final_pipeline = imbpipeline(steps=[(\"preprocessor\", preprocessor), \n",
    "                                   ('smote-sampler',SMOTE(random_state = 0)),\n",
    "                                   (\"feature_selection\",SelectKBest(mutual_info_classif, k = 'all')),\n",
    "                           (\"model\", model_pipeline)])\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "        final_pipeline.fit(X_train,  y_train)\n",
    "       \n",
    "\n",
    "        # Predict and calculate performance scores\n",
    "        y_pred = final_pipeline.predict(X_test)\n",
    "        performances.append([name,\n",
    "                             accuracy_score(y_test, y_pred),  # accuracy\n",
    "                             precision_score(y_test, y_pred, average=\"weighted\"),  # precisions\n",
    "                             recall_score(y_test, y_pred,average=\"weighted\"),  # recall\n",
    "                             f1_score(y_test, y_pred, average=\"weighted\")\n",
    "                             ])\n",
    "\n",
    "        # Print classification report\n",
    "        model_pipeline_report = classification_report(y_test, y_pred)\n",
    "        print(\"This is the classification report of the\",name, \"model\", \"\\n\", model_pipeline_report, \"\\n\")\n",
    "\n",
    "        # Defining the Confusion Matrix\n",
    "        model_pipeline_conf_mat = confusion_matrix(y_test, y_pred)\n",
    "        model_pipeline_conf_mat = pd.DataFrame(model_pipeline_conf_mat).reset_index(drop = True)\n",
    "        print(f\"Below is the confusion matrix for the {name} model\")\n",
    "\n",
    "        # Visualizing the Confusion Matrix\n",
    "        f, ax = plt.subplots()\n",
    "        sns.heatmap(model_pipeline_conf_mat, annot = True, linewidth = 1.0,fmt = \".0f\", cmap = \"RdPu\", ax=ax)\n",
    "        plt.xlabel = (\"Prediction\")\n",
    "        plt.ylabel = (\"Actual\")\n",
    "        plt.show()\n",
    "\n",
    "        # Store trained model\n",
    "        trained_model_name = \"trained_\" + str(name).lower()\n",
    "        trained_models[trained_model_name] = final_pipeline\n",
    "        \n",
    "        print(\"\\n\", \"-----   -----\"*6, \"\\n\",  \"-----   -----\"*6)\n",
    "    \n",
    "    # Compile accuracy\n",
    "    df_compare = pd.DataFrame(performances, columns = [\"model\", \"accuracy\", \"precision\", \"recall\", \"f1_score\"])\n",
    "    df_compare.set_index(\"model\", inplace = True)\n",
    "    df_compare.sort_values(by = [\"f1_score\", \"accuracy\"], ascending = False, inplace=True)\n",
    "    return df_compare, trained_models\n",
    "\n",
    "# %%\n",
    "# Run the function to train models and return performances\n",
    "all_models_eval, trained_models = evaluate_models()\n",
    "all_models_eval\n",
    "\n",
    "# %% [markdown]\n",
    "# #### `Visualizing Evaluation Using ROC - AUC`\n",
    "\n",
    "# %%\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "\n",
    "all_roc_data = {}\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for name,model in trained_models.items():\n",
    "    y_score = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test_encoded, y_score)\n",
    "\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    roc_data_df = pd.DataFrame({'False Positive Rate' : fpr , 'True Positve Rate' : tpr , 'Threshold' : thresholds})\n",
    "    all_roc_data[name] = roc_data_df\n",
    "\n",
    "    ax.plot(fpr,tpr, label = f'{name} (AUC = {roc_auc: .2f})')\n",
    "\n",
    "    ax.plot([0,1],[0,1], linestyle='--', color='k', label='Random')\n",
    "    ax.set_ylabel('False Positive Rate')\n",
    "    ax.set_xlabel('True Positive Rate')\n",
    "    ax.set_title('ROC Curve for all pipelines')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "\n",
    "lr_roc_data = all_roc_data[\"trained_logistic_regressor\"]\n",
    "svm_roc_data = all_roc_data[\"trained_svm\"]\n",
    "catboost_roc_data = all_roc_data[\"trained_catboost\"]\n",
    "xgboost_roc_data = all_roc_data[\"trained_xgboost\"]\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### `Business Impact Assessment`\n",
    "# \n",
    "# - The true positive rate is sensitive but there is a need to raise its sensitivity higher for production\n",
    "# - The acceptable threshold to meet the criteria is 0.4812 for the Logistic Regression model\n",
    "# - The acceptable threshold to meet the criteria is 0.3703 for the SVM model\n",
    "# - The acceptable threshold to meet the criteria is 0.2398 for the Cat Boost model\n",
    "# - The acceptable threshold to meet the criteria is 0.2189 for the Xgboost model\n",
    "# \n",
    "\n",
    "# %% [markdown]\n",
    "# #### `Hyperparameter Tuning`\n",
    "\n",
    "# %%\n",
    "## XGBoost Classifier\n",
    "xgb_clf = Pipeline(steps=[(\"preprocessor\", preprocessor), \n",
    "                          (\"model\", XGBClassifier(random_state=27))])\n",
    "\n",
    "# Defining the values for the RandomizedSearchCV\n",
    "param_grid_xgboost = {\"model__learning_rate\": [0.1, 0.3, 0.5, 0.7, 1.0],\n",
    "               \"model__max_depth\": [5, 10, 15, 20, 25, 30, 35],\n",
    "               \"model__booster\": [\"gbtree\", \"gblinear\", \"dart\"],\n",
    "               \"model__n_estimators\":  list(range(2, 11, 2))\n",
    "              }\n",
    "\n",
    "# %%\n",
    "# Running the RandomizedSearch Cross-Validation with the above set of Parameters\n",
    "grid_search_model = GridSearchCV(estimator = xgb_clf, param_grid = param_grid_xgboost, n_jobs=-1, scoring = \"accuracy\")\n",
    "\n",
    "\n",
    "# Fitting the model to the training data\n",
    "grid_search_model.fit(X_train,y_train_encoded)\n",
    "\n",
    "print(\"Best parameter (CV score=%0.5f):\" % grid_search_model.best_score_)\n",
    "print(f\"The best parameters for the GSCV XGB are: {grid_search_model.best_params_}\")\n",
    "\n",
    "# %%\n",
    "# Looking at the best combination of hyperparameters for the model\n",
    "best_gs_params = grid_search_model.best_params_\n",
    "print(\"The best combination of hyperparameters for the model will be:\")\n",
    "for param_name in sorted(best_gs_params.keys()):\n",
    "    print(f\"{param_name} : {best_gs_params[param_name]}\")\n",
    "\n",
    "# %%\n",
    "# Defining the best version of the model with the best parameters\n",
    "best_gs_model = Pipeline(steps=[(\"preprocessor\", preprocessor), \n",
    "                          (\"model\",XGBClassifier(random_state=27,\n",
    "                              booster=\"gblinear\",\n",
    "                              learning_rate=1.0,\n",
    "                              \n",
    "                              n_estimators=6\n",
    "                              ))])\n",
    "\n",
    "# Fit the model to the training data\n",
    "best_gs_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predict on the test data\n",
    "best_gs_pred = best_gs_model.predict(X_test)\n",
    "\n",
    "print(best_gs_pred)\n",
    "\n",
    "# %%\n",
    "# Confusion Matrix\n",
    "best_gs_conf_mat = (pd.DataFrame(confusion_matrix(y_test_encoded, best_gs_pred)).reset_index(drop=True))\n",
    "\n",
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(best_gs_conf_mat, annot=True, linewidth=1.0, fmt=\".0f\", cmap=\"RdPu\", ax=ax)\n",
    "\n",
    "# %%\n",
    "logistic_model = LogisticRegression(random_state=27)\n",
    "logistic_pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", logistic_model)])\n",
    "\n",
    "# Define the parameter distributions for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "        'model__penalty': ['l2'],\n",
    "    'model__solver' : ['lbfgs', 'liblinear', 'newton-cg'],\n",
    "    'model__max_iter' : [500,700,1000]\n",
    "}\n",
    "\n",
    "# Create the RandomizedSearchCV object\n",
    "clf = GridSearchCV(logistic_pipeline, param_grid=param_distributions, scoring = 'accuracy',error_score='raise')\n",
    "\n",
    "# Fit the RandomizedSearchCV on your training data\n",
    "search_model = clf.fit(X_train, y_train_encoded)\n",
    "\n",
    "print(\"Best parameter (CV score=%0.5f):\" % search_model.best_score_)\n",
    "print(f\"The best parameters for the GSCV XGB are: {search_model.best_params_}\")\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "# Predict on the test data\n",
    "search_pred = search_model.predict(X_test)\n",
    "\n",
    "# # Get the best hyperparameters\n",
    "# best_params = search.best_params_\n",
    "\n",
    "# print(best_params)\n",
    "# Defining the best version of the model with the best parameters\n",
    "best_search_model = Pipeline(steps=[(\"preprocessor\", preprocessor), \n",
    "                          (\"model\",LogisticRegression(random_state=27,\n",
    "                              max_iter=500,\n",
    "                              penalty='l2',\n",
    "                              solver = 'newton-cg',\n",
    "                              verbose=0\n",
    "                              ))])\n",
    "\n",
    "# Fit the model to the training data\n",
    "best_search_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predict on the test data\n",
    "best_search_pred = best_gs_model.predict(X_test)\n",
    "\n",
    "print(best_search_pred)\n",
    "\n",
    "# %%\n",
    "# Confusion Matrix\n",
    "best_search_conf_mat = (pd.DataFrame(confusion_matrix(y_test_encoded, best_search_pred)).reset_index(drop=True))\n",
    "\n",
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(best_search_conf_mat, annot=True, linewidth=1.0, fmt=\".0f\", cmap=\"RdPu\", ax=ax)\n",
    "\n",
    "# %%\n",
    "cat_model = (CatBoostClassifier(random_state=27))\n",
    "cat_pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", cat_model)])\n",
    "\n",
    "# Define the parameter distributions for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    #'model__C': uniform(scale=4),\n",
    "    'model__depth': [6],                   # Depth of the trees\n",
    "    'model__learning_rate': [0.1,1],          # Learning rate of the model\n",
    "    'model__l2_leaf_reg': [3],              # L2 regularization term on weights\n",
    "    'model__rsm': [0.2,0.8],                   # Random Selection Rate (regularization by introducing randomness)\n",
    "    'model__iterations': [500,800],            # Number of boosting iterations\n",
    "    'model__loss_function': ['MultiClass'], # Loss function for multi-class classification\n",
    "    'model__eval_metric': ['Accuracy'],    # Evaluation metric\n",
    "\n",
    "}\n",
    "\n",
    "# Create the RandomizedSearchCV object\n",
    "cat_clf = GridSearchCV(cat_pipeline, param_grid=param_distributions, scoring = 'accuracy',error_score='raise')\n",
    "\n",
    "# Fit the RandomizedSearchCV on your training data\n",
    "cat_gs_model = cat_clf.fit(X_train, y_train_encoded)\n",
    "\n",
    "print(\"Best parameter (CV score=%0.5f):\" % cat_gs_model.best_score_)\n",
    "print(f\"The best parameters for the GSCV XGB are: {cat_gs_model.best_params_}\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# Predict on the test data\n",
    "cat_gs__pred = search_model.predict(X_test)\n",
    "\n",
    "# # Get the best hyperparameters\n",
    "# best_params = search.best_params_\n",
    "\n",
    "# print(best_params)\n",
    "# Defining the best version of the model with the best parameters\n",
    "best_gs_catboost_model = Pipeline(steps=[(\"preprocessor\", preprocessor), \n",
    "                          (\"model\",CatBoostClassifier(random_state=27,\n",
    "                              iterations=500,\n",
    "                              depth=6,\n",
    "                              eval_metric = 'Accuracy',\n",
    "                              l2_leaf_reg=3,\n",
    "                              learning_rate=0.1,\n",
    "                              rsm = 0.8,\n",
    "                              loss_function = 'MultiClass',\n",
    "                              \n",
    "\n",
    "                              ))])\n",
    "\n",
    "# Fit the model to the training data\n",
    "best_gs_catboost_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predict on the test data\n",
    "best_catboost_pred = best_gs_catboost_model.predict(X_test)\n",
    "\n",
    "print(best_catboost_pred)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Confusion Matrix\n",
    "best_search_conf_mat = (pd.DataFrame(confusion_matrix(y_test_encoded, best_catboost_pred)).reset_index(drop=True))\n",
    "\n",
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(best_search_conf_mat, annot=True, linewidth=1.0, fmt=\".0f\", cmap=\"RdPu\", ax=ax)\n",
    "\n",
    "# %%\n",
    "svc_model = SVC(random_state=27)\n",
    "svc_pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", svc_model)])\n",
    "\n",
    "# Define the parameter distributions for GridSearchCV\n",
    "param_distributions = {\n",
    "    \n",
    "    'model__break_ties': [True],                   \n",
    "    'model__kernel': ['linear','rbf','poly'],         \n",
    "    'model__max_iter': [-1],              \n",
    "    'model__coef0': [0.0,0.2],                \n",
    "    'model__probability': [True ],           \n",
    "    'model__shrinking': [True,False], \n",
    "    'model__verbose': [True],    \n",
    "    'model__tol' : [0.0001,0.1]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "svc_clf = GridSearchCV(svc_pipeline, param_grid=param_distributions, scoring = 'accuracy',error_score='raise')\n",
    "\n",
    "# Fit the GridSearchCV on your training data\n",
    "svc_gs_model = svc_clf.fit(X_train, y_train_encoded)\n",
    "\n",
    "print(\"Best parameter (CV score=%0.5f):\" % svc_gs_model.best_score_)\n",
    "print(f\"\\nThe best parameters for the GSCV XGB are: {svc_gs_model.best_params_}\")\n",
    "\n",
    "# %%\n",
    "# Defining the best version of the model with the best parameters\n",
    "best_gs_svc_model = Pipeline(steps=[(\"preprocessor\", preprocessor), \n",
    "                          (\"model\",SVC(random_state=27,\n",
    "                              break_ties=True,\n",
    "                              coef0 = 0,\n",
    "                              kernel = 'linear',\n",
    "                              probability = True,\n",
    "                              max_iter =-1,\n",
    "                              shrinking = True,                                                      \n",
    "                              ))])\n",
    "\n",
    "# Fit the model to the training data\n",
    "best_gs_svc_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predict on the test data\n",
    "best_svc_pred = best_gs_svc_model.predict(X_test)\n",
    "\n",
    "print(best_svc_pred)\n",
    "\n",
    "# %%\n",
    "# Confusion Matrix\n",
    "best_svc_conf_mat = (pd.DataFrame(confusion_matrix(y_test_encoded, best_svc_pred)).reset_index(drop=True))\n",
    "\n",
    "# Visualizing the Confusion Matrix\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(best_search_conf_mat, annot=True, linewidth=1.0, fmt=\".0f\", cmap=\"RdPu\", ax=ax)\n",
    "\n",
    "# %% [markdown]\n",
    "# #### `Testing one of the models with the test data set`\n",
    "\n",
    "# %%\n",
    "test_data = pd.read_excel('data\\\\Telco-churn-last-2000.xlsx')\n",
    "test_data = cleaner(test_data)\n",
    "best_svc_pred = best_gs_svc_model.predict(test_data)\n",
    "\n",
    "print(best_svc_pred)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# #### `Persit the model`\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "for name, model_pipeline in all_models_pipelines.items():\n",
    "    joblib.dump(model_pipeline,f'models\\{name}.joblib')\n",
    "\n",
    "\n",
    "# %%\n",
    "joblib.dump(best_gs_pred ,'models\\\\tuned\\\\best_gs_pred .joblib')\n",
    "joblib.dump(best_search_pred,'models\\\\tuned\\\\best_search_pred.joblib')\n",
    "joblib.dump(best_catboost_pred,'models\\\\tuned\\\\best_catboost_pred.joblib')\n",
    "joblib.dump(best_svc_pred,'models\\\\tuned\\\\best_svc_pred.joblib')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
